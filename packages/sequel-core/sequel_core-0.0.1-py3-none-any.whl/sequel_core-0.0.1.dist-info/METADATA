Metadata-Version: 2.1
Name: sequel-core
Version: 0.0.1
Summary: A Continual Learning Framework for both Jax and PyTorch.
Author-email: Nikolaos Dimitriadis <nikolaos.dimitriadis@epfl.ch>
Project-URL: repository, https://github.com/nik-dim/sequel
Project-URL: homepage, https://nik-dim.github.io/sequel-site/
Project-URL: documentation, https://nik-dim.github.io/sequel-site/
Project-URL: issue-tracker, https://github.com/nik-dim/sequel/issues
Project-URL: releases, https://github.com/nik-dim/sequel/releases
Keywords: continual learning,pytorch,jax
Classifier: Development Status :: 3 - Alpha
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: beautifultable (==1.1.0)
Requires-Dist: coloredlogs (==15.0.1)
Requires-Dist: comet-ml (==3.31.21)
Requires-Dist: functorch (==1.13.1)
Requires-Dist: hydra-core (==1.3.2)
Requires-Dist: matplotlib (==3.6.2)
Requires-Dist: numpy (==1.23.4)
Requires-Dist: omegaconf (==2.2.3)
Requires-Dist: pandas (==1.5.1)
Requires-Dist: Pillow (==9.4.0)
Requires-Dist: pytest (==7.2.1)
Requires-Dist: torch (==1.13.1)
Requires-Dist: torchmetrics (==0.10.3)
Requires-Dist: torchvision (==0.14.1)
Requires-Dist: tqdm (==4.64.1)
Requires-Dist: wandb (==0.13.11)
Requires-Dist: tensorboard (==2.12.0)
Requires-Dist: gpustat (==1.0.0)
Requires-Dist: mkdocs (==1.4.2)
Requires-Dist: mkdocs-material (==9.1.2)
Requires-Dist: mkdocstrings (==0.20.0)
Requires-Dist: mkdocstrings-python (==0.8.3)
Requires-Dist: jax
Requires-Dist: chex (==0.1.5)
Requires-Dist: flax (==0.6.7)
Requires-Dist: optax (==0.1.4)
Requires-Dist: jax-resnet (==0.0.4)

# Sequel: A Continual Learning Library in PyTorch and JAX

The goal of this library is to provide a simple and easy to use framework for continual learning. The library is written in PyTorch and JAX and provides a simple interface to run experiments. The library is still in development and we are working on adding more algorithms and datasets.

- Documetation: https://nik-dim.github.io/sequel-site/
- Reproducibility Board: https://nik-dim.github.io/sequel-site/reproducibility/ 
- Weights&Biases: https://wandb.ai/nikdim/SequeL 
## Installation

The library can be installed via pip:
```bash
pip install sequel-core
```

Alternatively, you can install the library from source:
```bash
git clone https://github.com/nik-dim/sequel.git
python3 -m build
```

or use the library by cloning the repository. In order to use the library, you need to install the dependencies. This can be done via the `requirements.txt` file. We recommend to use a conda environment for this. The following commands will create a conda environment with the required packages and activate it:
```bash
# create the conda environment
conda create -n sequel -y python=3.10 cuda cudatoolkit cuda-nvcc -c nvidia -c anaconda
conda activate sequel 

# install all required packages
pip install -r requirements.txt

# Optional: Depending on the machine, the next command might be needed to enable CUDA support for GPUs
pip install jax[cuda11_cudnn82] -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
```


## Run an experiment

For some examples, you can modify the `example_pytorch.py` and `example_jax.py` files, or run:
```bash
# example experiment on PyTorch
python example_pytorch.py

# ...and in JAX
python example_jax.py
```

Experiments are located in the `examples/` directory in `configs`. In order to run an experiment you simply do the following:

```bash
python main.py +experiment=EXPERIMENT_DIR/EXPERRIMENT

# examples
python main.py +examples=ewc_rotatedmnist       mode=pytorch        # or mode=jax
python main.py +examples=mcsgd_rotatedmnist     mode=pytorch        # or mode=jax
```

In order to create your own experiment you follow the template of the experiments in `configs/examples/`. You override the defaults so that e.g. another algorithm is selected and you specify the training details. To run multiple experiments with different configs, the `--multirun` flag of [Hydra](https://hydra.cc/docs) can be used. 
For instance:
```bash
python main.py --multirun +examples=ewc_rotatedmnist \
     mode=pytorch optimizer.lr=0.01,0.001 \
     benchmark.batch_size=128,256 \ 
     training.epochs_per_task=1 # online setting
```
