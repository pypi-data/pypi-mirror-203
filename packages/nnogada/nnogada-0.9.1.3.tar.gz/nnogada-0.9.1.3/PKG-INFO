Metadata-Version: 2.1
Name: nnogada
Version: 0.9.1.3
Summary: Genetic hyperparameter tuning for neural nets
Home-page: https://github.com/igomezv/nnogada
Author: I Gomez-Vargas
Author-email: igomez@icf.unam.mx
License: MIT
Keywords: Hyperparameter,optimization,machine learning,deep learning,genetic algorithms
Platform: UNKNOWN
Classifier: Intended Audience :: Science/Research
Classifier: Operating System :: OS Independent
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.4
Classifier: Programming Language :: Python :: 3.5
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Natural Language :: English
License-File: LICENSE

Neural Networks Optimized by Genetic Algorithms for Data Analysis (NNOGADA) 

**nnogada** is a Python package that performs hyperparemeter tuning for artificial neural networks, particularly for Multi Layer Perceptrons, using simple genetic algorithms. Useful for generate better neural network models for data analysis. Currently, only works with feedforward neural networks in tensorflow.keras (classification and regression) and torch (regression at this moment).

You can try to install nnogada in your computer:

     $ git clone https://github.com/igomezv/nnogada

     $ cd nnogada

     $ pip3 install -e .

then you can delete the cloned repo because you must have nnogada installed locally.

Other way to install nnogada (without clonning) is:

    $ pip3 install -e git+https://github.com/igomezv/nnogada#egg=nnogada

If you use the code, please cite the paper *Gómez-Vargas, I., Andrade, J. B., & Vázquez, J. A. (2023). Neural networks optimized by genetic algorithms in cosmology. Physical Review D, 107(4), 043509.*

Contributions are welcome!



