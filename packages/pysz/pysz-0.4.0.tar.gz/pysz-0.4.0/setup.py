# -*- coding: utf-8 -*-
from setuptools import setup

package_dir = \
{'': 'src'}

packages = \
['pysz']

package_data = \
{'': ['*']}

install_requires = \
['numpy>=1.24.2,<2.0.0', 'pandas>=2.0.0,<3.0.0', 'zstandard>=0.20.0,<0.21.0']

setup_kwargs = {
    'name': 'pysz',
    'version': '0.4.0',
    'description': 'Package for reading and writing svb-zstd compressed data',
    'long_description': '# pysz\nPython interface for writing and reading svb-zstd compressed data\n\n## Installation\n\n### Dependencies\n\n- Python >= 3.8\n- Numpy >= 1.24.2\n- Pandas >= 2.0.0\n- Zstandard >= 0.20.0\n\n### Install using pip\n\n```bash\n# First, install a customized version of pystreamvbyte\npip install git+https://github.com/kevinzjy/pystreamvbyte.git\n\n# Install pysz now\npip install pysz\n```\n\n### Install from source\n\n```bash\n# First, install a customized version of pystreamvbyte\npip install git+https://github.com/kevinzjy/pystreamvbyte.git\n\n# Install pysz from source\ngit clone --recursive https://github.com/kevinzjy/pysz\ncd pysz\npip install .\n```\n\n## Usage\n\n### Create & Write SZ file \n\nSupported data typesï¼š\n\n- str\n- np.int16 / np.int32\n- np.uint16 / np.uint32\n- np.float16 / np.float32 / np.float64 / np.float128\n\nExample for creating SZ file\n\n```python\nimport numpy as np\nfrom pysz.api import CompressedFile\n\nheader = [(\'version\',  \'0.0.1\'), (\'date\', \'2023-04-03\')]\nattr = [(\'ID\', str), (\'Offset\', np.int32), (\'Raw_unit\', np.float32)]\ndatasets = [(\'Raw\', np.uint32), (\'Fastq\', str), (\'Move\', np.uint16)]\n\n# Create new SZ file\nsz = CompressedFile(\n    "/tmp/test_sz", mode="w",\n    header=header, attributes=attr, datasets=datasets,\n    overwrite=True, n_threads=8\n)\n\n# Save data in single-read mode \nfor i in range(10000):\n    sz.put(\n        f"read_{i}", # ID\n        0, # Offset\n        np.random.rand(), # Raw_unit\n        np.random.randint(70, 150, 4000), # Raw\n        \'\'.join(np.random.choice([\'A\', \'T\', \'C\', \'G\'], 450)), # Fastq\n        np.random.randint(0, 1, 4000), # Move\n    )\n\n# Save data in chunk mode\nfor i in range(100):\n    chunk = []\n    for j in range(100):\n        chunk.append((\n            f"read_{i}_{j}",\n            0,\n            np.random.rand(),\n            np.random.randint(70, 150, 4000),\n            \'\'.join(np.random.choice([\'A\', \'T\', \'C\', \'G\'], 450)),\n            np.random.randint(0, 1, 4000),\n        ))   \n    sz.put_chunk(chunk)\n\n# Remember to call it to ensure everything finished successfully\nsz.close()\n```\n\nExample for creating SZ file with multiprocessing\n\n> Note: for creating SZ file with multiprocessing, pass the `CompressedFile.q_in` (a multiprocessing.Manager().Queue() instance)\n> as parameter to parallelized function. Then use `queue.put((True, chunk))` and `queue.put((True, read))` for saving reads\n> or chunks directly.\n\n```python\nimport numpy as np\nfrom pysz.api import CompressedFile\nfrom multiprocessing import Pool\n\ndef writer(queue, chunk_id):\n    # Write in single-read mode\n    for i in range(10):\n        read = (\n            f"read_{chunk_id}_{i}",\n            0,\n            np.random.rand(),\n            np.random.randint(70, 150, 4000),\n            \'\'.join(np.random.choice([\'A\', \'T\', \'C\', \'G\'], 450)),\n            np.random.randint(0, 1, 4000),\n            np.random.random(4000),\n        )\n        queue.put((False, read))\n\n    # Write in chunk mode\n    chunk = []\n    for i in range(10):\n        chunk.append((\n            f"chunk_{chunk_id}_{i}",\n            0,\n            np.random.rand(),\n            np.random.randint(70, 150, 4000),\n            \'\'.join(np.random.choice([\'A\', \'T\', \'C\', \'G\'], 450)),\n            np.random.randint(0, 1, 4000),\n            np.random.random(4000),\n        ))\n    queue.put((True, chunk))\n\n\nheader = [(\'version\',  \'0.0.1\'), (\'date\', \'2023-04-03\')]\nattr = [(\'ID\', str), (\'Offset\', np.int32), (\'Raw_unit\', np.float32)]\ndatasets = [(\'Raw\', np.uint32), (\'Fastq\', str), (\'Move\', np.uint16)]\n\n# Create new SZ file\nsz = CompressedFile(\n    "/tmp/test_sz", mode="w",\n    header=header, attributes=attr, datasets=datasets,\n    overwrite=True, n_threads=8\n)\n\n# Use pool for multiprocessing\npool = Pool(8)\njobs = []\nfor i in range(10):\n    jobs.append(pool.apply_async(writer, (sz.q_in, i, )))\npool.close()\npool.join()\n\n# Remember to call it to ensure everything finished successfully\nsz.close()\n\n```\n\nExamples for reading SZ file\n\n```python\nimport numpy as np\nfrom pysz.api import CompressedFile\nsz = CompressedFile(\n        "/tmp/test_sz", mode="r", allow_multiprocessing=True,\n)\n\n# Get index information\nprint(sz.idx.head())\n\n# Get total read number\nprint(f"Loaded {sz.idx.shape[0]} reads")\n\n# Get the first read\nread = sz.get(0)\nprint(read.ID, read.Offset, read.Raw_unit, read.Raw, read.Fastq, read.Move)\n\n# Get first 10 reads\nreads = sz.get(np.arange(10))\n\n# Filter some reads\nidx = sz.idx.index[sz.idx[\'ID\'].isin([\'read_0\', \'read_1\'])]\nreads = sz.get(idx)\n```\n\nExample for reading SZ file chunk by chunk using multiprocessing\n\n> Note: for reading SZ file with multiprocessing, pass the chunked index as parameter, \n> and init CompressedFile instance with `allow_multiprocessing=True` separately in each process.\n\n```python\nimport numpy as np\nfrom pysz.api import CompressedFile\nfrom pysz.utils import error_callback\nfrom multiprocessing import Pool\n\ndef grouper(iterable, n, fillvalue=None):\n    """\n    Collect data info fixed-length chunks or blocks\n    grouper(\'ABCDEFG\', 3, \'x\') --> ABC DEF Gxx\n    """\n    from itertools import zip_longest\n    args = [iter(iterable)] * n\n    return zip_longest(*args, fillvalue=fillvalue)\n\ndef read_from_sz(file_name, idx):\n    """\n    Read from SZ inside each process\n    """\n    sz = CompressedFile(file_name, mode=\'r\', allow_multiprocessing=True)\n    reads = sz.get(idx)\n    sz.close()\n    return reads\n    \n# Init pool\nn_threads = 16\nchunk_size = 500\n\n# Get all reads stored in SZ file\nfile_name = \'/tmp/test_sz\'\nsz = CompressedFile(file_name, mode=\'r\')\nindex = sz.idx\nsz.close()\n\n# Add process to Pool\npool = Pool(n_threads)\njobs = []\nfor x in grouper(index.index, chunk_size):\n    chunk = [i for i in x if i is not None]\n    jobs.append(pool.apply_async(read_from_sz, (file_name, chunk, ), error_callback=error_callback))\npool.close()\n\n# Get output\nfor job in jobs:\n    ret = job.get()\n    # Process your data here\npool.join()\n```\n\nPlease refer to `tests/test_api.py` for detailed usage. \n\n## TODO\n\n- Add support for zfp for lossy float compression\n- Refactor the naive solution for using multiprocessing for writing SZ.',
    'author': 'Jinyang Zhang',
    'author_email': 'zhangjinyang@biols.ac.cn',
    'maintainer': None,
    'maintainer_email': None,
    'url': None,
    'package_dir': package_dir,
    'packages': packages,
    'package_data': package_data,
    'install_requires': install_requires,
    'python_requires': '>=3.8,<4.0',
}


setup(**setup_kwargs)
